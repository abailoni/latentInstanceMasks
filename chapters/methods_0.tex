% !TEX root = ../patchEmbeddings_review.tex

\section{The proposed model}\label{sec:model}

In this section, we present the architecture of the proposed model. In Sec. \ref{sec:self_masks} and Sec. \ref{sec:encoding_masks} we define local self-probability masks and show how we compressed them to a lower latent space by using a Variational Auto-Encoder. Then, in Sec. \ref{sec:model_and_loss} we present the main convolutional model and the employed training loss. Finally, in Sec. \ref{sec:multiscale_patches} we show how we predicted self-probability masks at different scales.

\subsection{Local self-probability masks}\label{sec:self_masks}
This work proposes to distinguish between different object instances based on the instance-aware pixel-pair affinity, which specifies whether two pixels belong to the same instance or not.
Given a pixel $i$ of the input image, then a set of affinities to neighboring pixels within a $M\times N$ window is learned (\textbf{see Fig. 2?}). We represent these affinities associated to pixel $i$ as a local self-probability mask, i.e. a matrix $\mathbf{m}_i \in \mathbb{R}^{M\times N}$ describing the probability for each neighbor $M\times N$ and pixel $i$ to be in the same instance.

In total, if the input image has $H\times W$ pixels, we predict $M\times N \times H \times W$ affinities. The training targets for these affinities can be derived from a ground-truth label image $\hat{L}$ according to:
\begin{equation}
\forall i\in H\times W, \, \forall j\in M\times N \qquad \hat{\mathbf{m}}_i[j] = 
\begin{cases}
1, \quad &\text{if } \hat{L}_i = \hat{L}_j \\
0, \quad &\text{if } \hat{L}_i \neq \hat{L}_j. 
\end{cases}
\end{equation}
Note that these definitions can be easily generalized from 2D to 3D.

\subsection{Encoding self-probability ground-truth masks}\label{sec:encoding_masks}
We first note that, among all feasible binary masks $\hat{\mathbf{m}} \in \{0,1\}^{M\times N}$, in practice only a part of them represent real occurring neighborhood structures. 
This suggests that it is possible to compress the masks to a lower dimensional space. 

In order to test this assumption, we then compressed binary ground-truth masks $\hat{\mathbf{m}}_i$ to latent variables $z_i\in \mathbb{R}^Q$ by training a convolutional Variational Auto-encoder (VAE) \cite{kingma2013auto,rezende2014stochastic} consisting of an encoder $p_{\phi}(z_i|\hat{\mathbf{m}}_i)$ and a decoder $p_{\phi}(\hat{\mathbf{m}}_i|z_i)$.
In our experiments, we evaluate how the dimension $Q$ of the latent space impacts the quality of the reconstructed binary masks and found in this way an optimal latent space dimension that is compact enough but at the same time preserves most of the information contained in the binary patches.

\subsection{Predicting encoded self-probability masks} \label{sec:model_and_loss}
Given an input image, the model proposed in this work is trained to predict for each pixel $i$ the latent vector $z_i\in \mathbb{R}^Q$ encoding the associated self-probability mask $\mathbf{m}_i$. Thus, if the input image has shape $H\times W$, the fully convolutional model outputs a feature map with shape $Q\times H\times W$. 
Note that this yields a much more compact model as compared to directly predicting all the $M\times N$ patches with a final output feature map of size $M\times N\times H\times W$.

Similarly to concurrent recent work \cite{hirsch2020patchperpix}, we tested two ways of training the proposed model. \\

\textbf{Training VAE and model separately} -- In the first case, we trained a VAE to encode ground-truth binary masks as explained above in Sec. \ref{sec:encoding_masks}. The main model was then trained to predict, for each pixel, the latent vector $z_i$ given by the encoded version of the ground truth binary mask predicted by the VAE model:
L2 loss between the predicted latent vector of the model $\tilde{z}_i$ and the encoded version of the associated ground truth binary mask $p_{\phi}()$
\begin{equation}
\mathcal{J}^A_{\mathrm{Loss}} = \sum_{i\in H\times W} ()^2
\end{equation}


\textbf{Training encoded self-probability masks end-to-end} --
We define the loss according to the S\o rensen-Dice coefficient \cite{dice1945measures,sorensen1948method} formulated for fuzzy set membership values:
\begin{equation}
\mathcal{J}^B_{\mathrm{Loss}} = \frac{\sum_{i\in H\times W} \sum_{j\in M\times N} (1-\mathbf{m}_i[j])(1-\hat{\mathbf{m}}_i[j])}{\sum_{i\in H\times W} \sum_{j\in M\times N} \big( (1-\mathbf{m}_i[j])^2 + (1-\hat{\mathbf{m}}_i[j])^2 \big)}
\end{equation} 
This type of loss, as compared to L2 or binary cross-entropy losses, is more robust against class imbalance and prediction/target sparsity.
Sampling some pixels...

\subsection{Multi-scale self-probability masks}\label{sec:multiscale_patches}
As main architecture, we used a (3D) U-Net \cite{ronneberger2015u, cciccek20163d} that is one of the most common choices for applications to biological and medical images \textbf{REFS}.
As we show in Fig. \ref{fig:main}, the model is trained to predict latent vectors $z_i$ at different depth levels of the UNet-decoder path...


