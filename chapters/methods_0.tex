% !TEX root = ../patchEmbeddings_review.tex

\section{Model and training strategy}\label{sec:model}
In this section, we first define \maskname masks and briefly review a common strategy to train affinities for a sparse neighborhood.
Then, in Sec. \ref{sec:encoding_masks}, we present our first main contribution, i.e. a model trained end-to-end to predict encoded \maskname mask, one for each pixel of the input image. 
% Finally, in Sec. \ref{sec:multiscale_patches}, we show how we predicted \maskname masks at different scales.

\subsection{Local \maskname masks}\label{sec:self_masks}
This work proposes to distinguish between different object instances based on instance-aware pixel-pair affinities, which specifies whether two pixels belong to the same instance or not.
Given a pixel of the input image with coordinates $\coord{u}= (u_x, u_y)$, then a set of affinities to neighboring pixels within a $K\times K$ window is learned, where $K$ is an odd number. 
We define the $K\times K$-neighborhood of a pixel as:
\begin{equation}
\mathcal{N}_{K\times K} \equiv \mathcal{N}_{K} \times \mathcal{N}_{K}, \qquad \text{where} \quad \mathcal{N}_{K} \equiv \left\{-\frac{K-1}{2}, \ldots, \frac{K-1}{2}\right\}
\end{equation}
and represent the affinities associated to pixel $\coord{u}$ as a \maskname mask, i.e. a function $\mathcal{M}_{\coord{u}}: \mathcal{N}_{K\times K} \rightarrow [0,1]$ describing the probability for each neighboring pixel in the patch to be in the same instance associated to the central pixel $\coord{u}$.

% In total, if the input image has $H\times W$ pixels, we predict $K^2 \times H \times W$ affinities. 
We represent the associated training targets as binary ground-truth masks $\hat{\mathcal{M}}_{\coord{u}}: \mathcal{N}_{K\times K} \rightarrow \{0,1\}$, which can be derived from a ground-truth instance label image $\hat{L}: H\times W \rightarrow \mathbb{N}$ according to:
\begin{equation}\label{eq:target_masks}
\forall\, \coord{u}\in H\times W, \quad \forall\, \coord{n}\in \mathcal{N}_{K\times K} \qquad \hat{\mathcal{M}}_{\coord{u}}(\coord{n}) = 
\begin{cases}
1, \quad &\text{if } \hat{L}(\coord{u}) = \hat{L}(\coord{u}+\coord{n}) \\
0, \quad & \text{otherwise},
\end{cases}
\end{equation}
where $H\times W$ is the size of the input image. Note that these definitions can be easily generalized to the 3D case.

\subsection{Training affinities for a given sparse neighborhood}\label{sec:affs_from_sparse}
In this section, we briefly overview a recent training method that has been widely used in instance segmentation \cite{liu2018affinity,Gao_2019_ICCV,lee2017superhuman,wolf2018mutex,bailoni2019generalized} and we also use to train our model (see \emph{sparse-neighborhood branch} in Fig. \ref{fig:main_figure}a).

In this training setup, affinities between pairs of pixels are predicted for a predefined sparse stencil representing a set of $N$ short- and long-range neighborhood relations for each pixel ($N=8$ in Fig. \ref{fig:main_figure}a). In other words, we do not predict one full \maskname mask for each pixel of the input image, but only $N$ pixels of each \maskname mask. 
As shown in the \emph{sparse-neighborhood branch} of Fig. \ref{fig:main_figure}a, the backbone model is trained to output $N$ feature maps, each representing a different neighborhood relation. We then define a dense channel-wise loss for every pixel according to the S\o rensen-Dice coefficient \cite{dice1945measures,sorensen1948method} (see \cite{wolf2018mutex} for details).
As compared to L2 or binary cross-entropy, this loss is more robust against prediction and / or target sparsity, a desirable quality for our application in neuron segmentation, since neuron membranes occupy very few pixels in three-dimensional volume.

\subsection{Training encoded \maskname masks end-to-end}\label{sec:encoding_masks}
The training method presented above in Sec. \ref{sec:affs_from_sparse} can be easily modified to output a feature map of size $K^2 \times H \times W$ and predict then a full $K\times K$ \maskname mask for each pixel of the input image (see \emph{mask-per-pixel branch} in Fig. \ref{fig:main_figure}b).
However, this model would come with a high GPU-memory cost and would not scale for large values of $K$, especially for applications on 3D images.

We then note that, among all feasible binary masks $\hat{\mathcal{M}}_{\coord{u}}: \mathcal{N}_{K\times K} \rightarrow \{0,1\}$, in practice only a part of them represent real occurring neighborhood structures \TODO{Fig?}. 
This suggests that it is possible to compress the masks to a lower dimensional space. 

As our first main contribution, we then test this assumption by training a model end-to-end to predict, for each pixel $\coord{u}\in H\times W$ of the input image, a latent vector $z_{\coord{u}}\in \mathbb{R}^Q$ encoding the $K \times K$ \maskname mask $\mathcal{M}_{\coord{u}}$ centered at pixel $\coord{u}$ (see Fig. \ref{fig:main_figure}c). 
In more details, the backbone model is trained to output a much more compact $Q\times H\times W$ feature map. 
% as compared to $K^2 \times H \times W$. 
Then, a tiny convolutional decoder network is applied to each single pixel of the feature map to obtain \maskname masks (see \emph{mask-per-pixel-decoder} in Fig. \ref{fig:main_figure}c).

During training, decoding all predicted encoded masks $z_{\coord{u}}$ for all pixels in the image would be too memory consuming. Thus, we randomly sample $R$ pixels with coordinates $\coord{u}_1, \ldots, \coord{u}_R$ and only decode the associated masks $\mathcal{M}_{\coord{u}_1}, \ldots, \mathcal{M}_{\coord{u}_R}$. 
Given the ground-truth \maskname masks $\hat{\mathcal{M}}_{\coord{u}_i}$ defined in Eq. \ref{eq:target_masks}, the training loss is then defined according to the S\o rensen-Dice coefficient formulated for fuzzy set membership values:
\begin{equation}
\mathcal{J}_{\mathrm{mask-loss}} = \frac{\sum_{i=1}^R \sum_{\coord{n} \in \mathcal{N}_{K\times K}} \big(1-\mathcal{M}_{\coord{u}_i}(\coord{n})\big)\cdot \big(1-\hat{\mathcal{M}}_{\coord{u}_i}(\coord{n})\big)}{\sum_{i=1}^R \sum_{\coord{n}\in \mathcal{N}_{K\times K}} \Big( \big(1-\mathcal{M}_{\coord{u}_i}(\coord{n})\big)^2 + \big(1-\hat{\mathcal{M}}_{\coord{u}_i}(\coord{n})\big)^2 \Big)}.
\end{equation} 
Ground-truth labels are not always pixel-precise and it is often impossible to estimate the correct label for pixels that are really close to a ground-truth label transition. Thus, in order to avoid noise during training, we predict completely empty masks for pixels that are less than two pixels away from a label transition. 


\subsection{Predicting multi-scale \maskname masks}\label{sec:multiscale_patches}
Previous related work both on neuron segmentation \cite{lee2017superhuman} and natural images \cite{liu2018affinity,Gao_2019_ICCV} shows that using the training strategy presented in Sec. \ref{sec:affs_from_sparse} to predict long-range affinities between distant pixels improves performances as compared to predicting only short-range ones. However, in the new training strategy proposed in Sec. \ref{sec:encoding_masks}, predicting large \maskname masks would translate in a bigger model that, on 3D data, would have to be trained on a smaller 3D input.
This, in practice, usually decreases performances because of the reduced surrounding 3D context provided during training, which is an important hyper-parameter in neuron segmentation.
% whose size is an important hyper-parameter in neuron segmentation since the correct detection of mitochondria and other organelles strongly depends on how much surrounding 3D context is provided during training.

Thus, we instead follow the approach of \cite{Gao_2019_ICCV} and predict multiple \maskname masks of the same window size $5\times 7 \times 7$ but at different resolutions, so that the lower the resolution the larger the size of the associated patch in the input image  (\TODO{fig}). 
% Note that the size of \maskname masks in this work is kept fixed only for simplicity reasons.
These multiple masks at different resolutions are predicted by adding several mask-decoder branches along the hierarchy of the decoder in the backbone model, which in our case is a 3D U-Net \cite{ronneberger2015u,cciccek20163d}. 
In this way, the encoded \maskname masks at higher and lower resolutions can be effectively learned at different feature levels in the feature pyramid of the U-Net, as it was already shown in previous work on object detection and instance segmentation \cite{Gao_2019_ICCV,lin2017feature}.



