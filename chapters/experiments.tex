% !TEX root = ../patchEmbeddings_review.tex

\section{Experiments on neuron segmentation}
\subsection{Data: CREMI challenge} \label{sec:cremi_challenge}
We evaluate and compare our method on the task of neuron segmentation in electron microscopy (EM) image volumes. This application is of key interest in connectomics, a field of neuro-science with the goal of reconstructing neural wiring diagrams spanning complete central nervous systems. Currently, only proof-reading or manual tracing yields sufficient accuracy for correct circuit reconstruction \cite{schlegel2017learning}, thus further progress is required in automated reconstruction methods.

We test our method on the competitive CREMI 2016 EM Segmentation Challenge \cite{cremiChallenge} that is currently the neuron segmentation challenge with the largest amount of training data available. The dataset comes from serial section EM of \emph{Drosophila} fruit-fly tissue and consists of 6 volumes of $1250\times 1250\times 125$ voxels at resolution $4\times 4\times 40$ nm$^3$, three of which come with publicly available training ground truth. 
We achieved the best scores by downscaling the resolution of the EM data of a factor $(\frac{1}{2},\frac{1}{2},1)$, since this helped increasing the 3D context provided as input to the model.
We use the second half of CREMI sample C as validation set for our comparison experiments in Table \ref{tab:val_results} and then we train a final model on all the three samples with available ground truth labels to submit results to the leader-board in Tab. \ref{tab:test_results}. 
Results  are evaluated using the CREMI score, which is given by the geometric mean of Variation of Information Score (VOI split + VOI merge) and Adapted Rand-Score (Rand-Score), two popular metrics used to evaluate clusterings \cite{arganda2015crowdsourcing}.

\textbf{Data augmentation} -- The data from the CREMI challenge is highly \linebreak anisotropic and contains artifacts like missing sections, staining precipitations and support film folds. 
To alleviate difficulties stemming from misalignment, we use a version of the data that was elastically realigned by the challenge organizers with the method of \cite{saalfeld2012elastic}.
In addition to the standard data augmentation techniques of random rotations, random flips and  elastic deformations, we simulate data artifacts.
In more detail, we randomly zero-out slices, introduce alignment jitter and paste artifacts extracted from the training data. Both \cite{funke2018large} and \cite{lee2017superhuman} have shown
that these kinds of augmentations can help to alleviate issues caused by EM-imaging artifacts. For zero-out slices, the model is trained to predict the ground-truth labels of the previous slice.
On the test samples, we run predictions for overlapping volumes and then average them.


 
 


\subsection{Architecture of the tested models}
Similarly to \cite{lee2019learning,lee2017superhuman,wolf2018mutex}, as a backbone model we use a 3D U-Net consisting of a hierarchy of four feature maps with anisotropic downscaling factors $(\frac{1}{2},\frac{1}{2},1)$. 
% U-Net with three levels in the feauter map hierarchy, involving downscaling factors $(1, \frac{1}{2},\frac{1}{2})$
Models are trained with Adam optimizer and batch size equal to one. Before to apply the loss, we slightly crop the predictions to prevent training on borders where not enough surrounding context is provided. 
See \TODO{supplementary material} for all details about the used architecture. 

 
\textbf{Baseline model (SNB)} -- As a strong baseline, we train a model to predict affinities for a sparse neighborhood structure (Fig. \ref{fig:main_figure}a). We perform deep supervision by attaching three \emph{\sparseBr branches} (SNB) at different levels in the hierarchy of the UNet decoder and train the coarser feature maps to predict longer range affinities. Details about the used neighborhood structures can be find in \TODO{supplementary material}.

\textbf{Proposed model (ENB)} -- We then train a model to predict encoded \maskname masks (Fig. \ref{fig:main_figure}c). Similarly to the baseline model, we provide deep supervision by attaching four \emph{\encBr branches} (ENB) to the backbone U-Net. As explained in Sec. \ref{sec:multiscale_patches}, all branches predict 3D masks of shape $7 \times 7 \times 5$, but at different resolutions $(1,1,1)$, $(\frac{1}{4},\frac{1}{4},1)$ and $(\frac{1}{8},\frac{1}{8},1)$, as we show in \TODO{supplementary material}.

\textbf{Combined model (SNB+ENB)} -- Finally, we also train a combined model to predict both \maskname masks and a sparse neighborhood of affinities, by providing deep supervision both via \emph{\encBr} and \emph{\sparseBr} \emph{branches}. The backbone of this model is then trained with a total of seven branches: three branches equivalent to the ones used in the baseline model SNB, plus four additional ones like those in the ENB model.  

\subsection{Graph-partitioning methods} 
Given the output of the compared models, we compute affinities $a_e$ either with the efficient method of Sec. \ref{sec:efficient_affs} or the average aggregation introduced in Sec. \ref{sec:aggr_affs} (\textbf{AffAggr}) and then use them as edge weights for a pixel grid-graph with the neighborhood structure given in \TODO{supplementary material}. Positive and negative edge weights $w_e$ are computed by applying the additive transformation $w_e=a_e-0.5$ to the affinities.

In order to obtain final instances, we test different partitioning algorithms.
 % that have been already applied to neuron segmentation. 
The Mutex Watershed (\textbf{MWS}) \cite{wolf2018mutex} is an efficient algorithm to partition graphs with both attractive and repulsive weights without the need of extra parameters. It can easily handle the large graphs considered here with up to $10^8$ nodes/voxels and $10^9$ edges\footnote{Among all edges given by the chosen neighborhood structure, we add only 10\% of the long-range ones, since the Mutex Watershed was shown to perform optimally in this setup \cite{bailoni2019generalized,wolf2018mutex}.}. However, on a challenging problem like the one considered here where the predictions of the model are not always consistent, the MWS can sometimes lead to over-segmentation and under-segmentation artifacts in the final segmentation because of the greedy nature of the algorithm.

Thus, in our comparison we also include another graph partitioning pipeline that has been often applied to neuron segmentation because of its robustness. This method first generates a 2D super-pixel over-segmentation from the model predictions and then partition the associated region-adjacency graph to obtain final instances. Super-pixels are computed with the following procedure: first, the predicted direct-neighbor affinities are averaged over the two isotropic directions to obtain a 2D neuron-membrane probability map; then, for each single 2D image in the stack, super-pixels are generated by running a watershed algorithm seeded at the maxima of the boundary-map distance transform (\textbf{WSDT}). Given this initial over-segmentation, a 3D region-adjacency graph is built, so that each super-pixel is represented by a node in the graph. Edge weights of this graph are computed by averaging short- and long-range affinities over the boundaries of neighboring super-pixels. 
Finally, the graph is partitioned by either solving the multicut optimization problem (\textbf{MC}) or by applying the average agglomeration algorithm proposed in \cite{bailoni2019generalized} (\textbf{GaspAverage}).

\TODO{In the supplementary material, we describe how we processed glia processes (putative astrocyte) separately, similarly to other related work \cite{lee2019learning}.}  
% GaspAverage is more robust to noise as compared to Mutex Watershed \cite{bailoni2019generalized}, but it is considerably more computationally expensive on large graphs like the ones considered here (up to $10^8$ nodes/voxels and $10^9$ edges). Thus, in our comparison on the validation set, we also... 

  



\begin{table}[t]
\small
\centering
  % {\renewcommand{\arraystretch}{1.3}
  % \resizebox{\textwidth}{!}{
        \begin{tabular}[t]{l L{1em} L{10em}c c}
         % Method & \makecell{CREMI-Score \\(lower is better)} \\ \midrule 
\thead{Partitioning algorithm} & & \thead[l]{Type of model} & \thead{CREMI-Score \\(lower is better)} & \thead{VI-merge \\(lower is better)} \\ \toprule 
% % & & SNB & 1.144 & 0.875 \\ 
% & & SNB+ENB & \textbf{0.115} & 0.222 \\
% & & SNB & 0.119 & \textbf{0.219} \\
% GaspAverage  & & SNB+ENB+AffAggr &  0.156 & 0.277 \\
% & &ENB & 0.173 & 0.242 \\
% & &ENB+AffAggr & 0.188 & 0.281 \\ \midrule
& &SNB+ENB & \textbf{0.130} & \textbf{0.234} \\
WSDT+GaspAverage & & SNB & 0.147 & 0.258 \\
&& ENB & 0.173 & 0.240 \\ \midrule
& &SNB+ENB & \textbf{0.135} & \textbf{0.235} \\
WSDT+MC & & SNB &  0.151 & 0.258 \\
& & ENB & 0.178 & 0.240 \\ \midrule
& & SNB+ENB+AffAggr & \textbf{0.155} & 0.278 \\
& & ENB+AffAggr & 0.183 & \textbf{0.275} \\
MutexWatershed & & ENB & 0.417 & 0.311 \\
& & SNB+ENB &  0.531 & 0.449 \\
% & & SNB+ENB & 0.539 & 0.376 \\
& & SNB & 0.895 & 0.634 \\ 
% Superpixels without long-range:
% UNet+SNB+ENB & WSDT+GASP-Avg & 0.137 & 0.260 \\
% UNet+SNB & WSDT+GASP-Avg & 0.188 & 0.333 \\
% UNet+ENB & WSDT+GASP-Avg & 0.197 & 0.299 \\
% Multicut without long range:
% UNet+SNB+ENB & WSDT+MC & 0.127 & 0.245 \\
% UNet+ENB & WSDT+MC & 0.156 & 0.248 \\
% UNet+SNB & WSDT+MC & 0.183 & 0.317 \\
% Superpixels ong AffAggr
% UNet+SNB+ENB+AffAggr & WSDT+GASP-Avg-LR & 0.219 & 0.309 \\
% UNet+SNB+ENB+AffAggr & WSDT+GASP-Avg & 0.229 & 0.316 \\
% UNet+ENB+AffAggr & WSDT+GASP-Avg-LR & 0.243 & 0.316 \\
% UNet+ENB+AffAggr & WSDT+GASP-Avg & 0.248 & 0.323 \\
        \end{tabular}
        % }
        \vspace{1em}
        \caption{Comparison experiments on our CREMI validation set} \label{tab:val_results}
\end{table}

% \begin{figure}[t]
%         \centering
% \begin{minipage}[t]{0.49\textwidth}
%     \centering
%     % \scriptsize
%         \begin{tabular}[t]{l|c}
%          Method & \makecell{CREMI-Score \\(lower is better)} \\ \midrule 
% \textbf{} \textbf{Average}& \textbf{0.226}  \\
%  Sum + Constraints \cite{levinkov2017comparative} & 0.282 \\
%  Abs. Max. \cite{wolf2018mutex} & 0.322 \\
%  Max. + Constraints & 0.324 \\
%  Sum \cite{keuper2015efficient} & 0.334 \\
%  Average + Constraints & 0.563 \\
% THRESH & 1.521 \\ 
%         \end{tabular}
%     % \captionof{table}{CREMI-Scores achieved by different linkage criteria and thresholding. All methods use the affinity predictions from our CNN as input. Scores are averaged over the three CREMI training datasets.}
%     \label{tab:results_cremi_train}
% \end{minipage}\hfill
% % \begin{minipage}[t]{0.35\textwidth}
% % \centering
% %     \scriptsize
% %     \vspace*{-1.5em}
% % \begin{tabular}[t]{l|cc}
% %         \multirow{2}{*}{Method}    & AP  & AP 50\% \\ 
% %          & \multicolumn{2}{c}{(higher is better)} \\ \midrule
% %            Panoptic-DeepLab \cite{cheng2019panopticdeeplab} & 34.6 & 57.3 \\
% %            UPSNet \cite{xiong2019upsnet} $\dagger$ & 33.0 & 59.6 \\
% %            SSAP \cite{Gao_2019_ICCV} & 32.7 & 51.8 \\
% %            AdaptIS \cite{sofiiuk2019adaptis} & 32.5 & 52.5 \\
% %            PANet \cite{liu2018path} $\dagger$ & 31.8 & 57.1 \\
% %            \textbf{GMIS Model \cite{liu2018affinity} +  Average} & \textbf{28.3} & \textbf{47.0} \\ 
% %            JOSECB \cite{neven2019instance} & 27.7 & 50.9 \\
% %            \textbf{GMIS} \cite{liu2018affinity} & \textbf{27.3} & \textbf{45.6} \\
% %            Mask R-CNN \cite{he2017mask} $\dagger$ & 26.2 & 49.9 \\
% %            SGN \cite{liu2017sgn} & 25.0 & 44.9 \\
% %            % DIN \cite{arnab2017pixelwise} & 20.0 & 38.8 \\
% %            % DWT \cite{bai2017deep} & 19.4 & 35.3 \\
% %            % InstanceCut \cite{kirillov2017instancecut} & 13.0 & 27.9 \\
% %         \end{tabular}
% %     % \caption{CityScapes \emph{test} set}
% %     % \vspace*{0.6em}
% %     \captionof{table}{Results on CityScapes test. Methods marked with~$\dagger$ are \emph{proposal-based}. Only methods that do not use external training data (such as MS COCO) are shown.}\label{tab:results_cityscapes}
% %     \label{tab:results_cityscapes_test}
% % \end{minipage}
% \end{figure}

\subsection{Results and discussion}

\textbf{Pre-training of the encoded space} -- The proposed model based on a \emph{\encBr branch} can be properly trained only if the dimension $Q$ of the latent space is large enough to encode all possible occurring neighborhood patterns. 
To make sure of this, we then trained a convolutional Variational Auto-encoder (VAE) \cite{kingma2013auto,rezende2014stochastic} to compress binary ground-truth \maskname masks $\hat{\mathcal{M}}_{\coord{u}}$ into latent variables $z_{\coord{u}}\in \mathbb{R}^Q$ and evaluated the quality of the reconstructed binary masks via the reconstruction loss. We then concluded that $Q=32$ was large enough to compress the masks considered here consisting of $7\times 7 \times 5=245$ pixels. 

We then tried to make use of this pre-trained encoded space to train the proposed ENB model and predict encoded masks directly in it by using a L2 loss on the encoded vectors. However, similarly to the findings of \cite{hirsch2020patchperpix}, this approach performed worse than directly training the full model end-to-end as described in Sec. \ref{sec:encoding_masks}. \\
% \begin{itemize}
% \item Mention how we tested the latent space dimension by training a VAE (or AE) to compress binary ground-truth \maskname masks: \emph{We then test this assumption by compressing binary ground-truth \maskname masks $\hat{\mathcal{M}}_{\coord{u}}$ to latent variables $z_{\coord{u}}\in \mathbb{R}^Q$ by training a convolutional Variational Auto-encoder (VAE) \cite{kingma2013auto,rezende2014stochastic} consisting of an encoder $p_{\phi}(z_{\coord{u}}|\hat{\mathcal{M}}_{\coord{u}})$ and a decoder $p_{\phi}(\hat{\mathcal{M}}_{\coord{u}}|z_{\coord{u}})$.
% In our experiments, we evaluate how the dimension $Q$ of the latent space impacts the quality of the reconstructed binary masks and find in this way an optimal latent space dimension that is compact enough but at the same time preserves most of the information contained in the binary masks.}
% \item Mention that we also tried to pre-train the encoded space with a VAE, but this did not work better than directly training the space end-to-end (similarly to PatchPerPix). 
% \emph{In this case, we first train a VAE to encode ground-truth binary masks as explained above in Sec. \ref{sec:encoding_masks}. 
% The main backbone model is then trained to predict, for each pixel $\coord{u}$, the mean and the standard deviation of the encoded distribution $p_{\phi}(z_{\coord{u}}|\hat{\mathcal{M}}_{\coord{u}})$ predicted by the pre-trained encoder, where $\hat{\mathcal{M}}_{\coord{u}}$ is the ground truth \maskname mask associated to pixel $\coord{u}$. An L2 loss is used to pull the two encoded vectors close to each other's. 
% The reasoning behind this approach is to train the backbone model to predict the masks in a meaningful compressed latent space. 
% Nevertheless, as we will show in our experiments, this method was the least successful among the tested ones (\emph{similarly to the findings of \cite{hirsch2020patchperpix}}).}
% \end{itemize}



\textbf{Training both affinities and encoded masks} -- As we show in our validation experiments in Table \ref{tab:val_results}, the model that achieved the best scores in all the tested setups is the combined one (ENB+SNB) predicting both encoded \maskname masks and affinities for a sparse neighborhood structure. 
% Predicting only affinities for a sparse-neighborhood, like the SBN model, is an easier task as compared to predicting large and dense neighborhood structures. 
We argue that, as compared to the baseline model SNB, the combined one  is encouraged to predict only occurring neighborhood patters through the mask-encoding process and acquire in this way prior knowledge about the shape of the segments, which can be helpful to correctly segment the more difficult regions in the data. 
% Thus, we argue that the combined model is less likely to result in a basic boundary detector based on simple local features and it is instead more likely to learn more complex features and use for example prior information about the shape of the segments.
 % like in the models ENB and ENB+SBN, is a more challenging task than predicting affinities for a sparse-neighborhood, like for the SBN model. Thus, we argue that the models predicting \maskname masks are 
 % and instead are encouraged to learn long-range features that can be helpful in the most challenging cases.
% Moreover, ... 

We also observe that training only \maskname masks (ENB model) did not perform as well as the state-of-the-art method predicting affinities (SNB model). A possible explanation of this is that the ENB model, as explained in Sec. \ref{sec:encoding_masks}, is trained with a sparse gradient for a small fraction of all the predicted encoded masks (due to GPU-memory restrictions), whereas the SNB model receives gradient for every pixel in the output feature map. 
On the other hand, the combined model ENB+SNB, which receives dense gradient from the \emph{\sparseBr branch}, predicted sharper and more accurate \maskname mask as compared to the ENB model. Thus, to summarize, our results show that the combined model trained to predict both affinities and \maskname masks performs better than the models trained to solve the two tasks singularly, which is a common behavior often observed in supervised learning. \\
% Not only improves affinities, but also the opposite. 

\textbf{Affinities from aggregated masks}  -- 
In our validation experiments, we also test the affinities computed by averaging over overlapping masks, as described in Sec. \ref{sec:aggr_affs}. We then partition the resulting signed graph, i.e. a graph with both positive and negative weights, by using the Mutex Watershed algorithm, which is parameter-free, very simple to implement and have empirical linearithmic complexity in the number of edges (see scores in Table \ref{tab:val_results}). 

We first note that the Mutex Watershed, for the first time on this type of more challenging neuron segmentation data, achieves comparable scores to the super-pixels-based methods, which have been always known to be more robust to noise but also require the user to tune more hyper-parameters.   
In particular, the Mutex Watershed performs the best with the combined model ENB+SNB, but only when affinities are computed from aggregated masks (AffAggr).
This can be explained by considering two facts. First, the MWS algorithm is very greedy, i.e. it merges / constrains clusters according to the most attractive / repulsive weights in the graph. And second, most of the affinities predicted by the \emph{\sparseBr branch} (SNB) are almost binary when the training with the classification loss has converged, i.e. in other words they present values either really close to zero or really close one\footnote{This is particularly true when the binary classification loss is based on the S\o rensen-Dice coefficient, as we explain in Sec. \ref{sec:affs_from_sparse}.}. 
Thus, the MWS algorithm can take full advantage of the \maskname aggregation process by assigning a higher priority to the edges with low/high affinities that were more consistently predicted across overlapping masks.
% Thus, we first note that these affinities are substantially different from those computed by averaging over overlapping masks as described in Sec. \ref{sec:aggr_affs} \TODO{Fig?}.
 % This fact is also shown in Table \ref{tab:val_results} by the validation scores achieved by the Mutex Watershed.

On the other hand, the super-pixels-based methods tested here have been designed and tailored to the standard binary classification output of a deep learning model, so they do not perform equally well on affinities computed from aggregated masks.\\



\textbf{Results on test samples} -- 



\begin{table}[t]
\centering
\begin{minipage}[t]{0.65\textwidth}
    \centering
    % \scriptsize
        \begin{tabular}[t]{l|c}
        Method & \makecell{CREMI-Score \\(lower is better)}  \\ \midrule
UNet + SNB + WSDT + LMC \cite{bailoni2019generalized} &  0.221\\
PNI-UNet + SNB \UPDATE{+ WSDT + GaspAverage} \cite{lee2017superhuman} & 0.228 \\
UNet + SNB + GaspAverage \cite{bailoni2019generalized} & 0.241 \\
MALA-UNet + WSDT + MC \cite{funke2018large} & 0.276 \\
CRU-Net \cite{zeng2017deepem3d} & 0.566  \\
LFC \cite{parag2017anisotropic} & 0.616  \\
        \end{tabular}
        \vspace*{0.99em}
    \caption{Current leading entries  in the CREMI challenge leaderboard \cite{cremiChallenge} (March 2020)}
    \label{tab:test_results}
\end{minipage}
\end{table}


